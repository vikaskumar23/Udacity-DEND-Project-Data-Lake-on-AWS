## Udacity Data Engineering Project : Data Lake on AWS
The objective of this project is to build a data lake and an ETL pipeline in Spark that loads data from S3, processes the data into analytics tables, and loads them back into S3. This will allow the analytics team to identify which songs users are listening to and how people are interacting with the sparkify music listening app.

## Introduction
In this project, a spark job is created that takes data from Amazon S3 bucket, process it and convert it to Star Schema data model. Then data is finally stored in Amazon S3 bucket.

## Project Scope
Data Lake is created on AWS that contains both processed and unprocessed data. The data can be used by analysts, business people as per their requirement.
The analytics Team wants to analyze the behaviour of users on sparkify app.

Following things can be analyzed:
1. Most heared song
2. Most liked artist
3. Usage and Artist Distribution by country
4. No. of paid subscribers
5. Distribution of gender in users
6. Where more money needs to be invested to increase the engagement of users
... and many more

## Data Required for Project
#### Song Dataset
The first dataset is a subset of real data from the [Million Song](https://labrosa.ee.columbia.edu/millionsong/) Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset:
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```
#### Log Dataset
The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset:
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```
## Data Model
Using the song and log datasets, data is transformed into Star Schema, so that it can be queries easily and understandable to analysts and business users.
First Data is fetched from S3 bucket to staging tables.
![Staging Taables](https://github.com/vikaskumar23/Udacity-DEND-Project-Data-Warehouse-on-Redshift/blob/master/staging_tables.PNG)

Then after some transformations data is loaded into star Schema. Some transformations include:
- Date data is divided in chunks to create date dimension.
- Songs data and events data is joined to create songplays fact data.
- Duplicate data is removed before transforming to star schema.

The resulting star schema consists of one fact table and four dimension tables.
![Data Model](https://github.com/vikaskumar23/Udacity-DEND-Project-Data-Warehouse-on-Redshift/blob/master/dbmodel.png)
##### Fact Table
- **songplays:** records in log data associated with song plays i.e. records with page ```NextSong```
    - **songplay_id:** unique id for each songplay event
    - **start_time:** start time of event (timestamp)
    - **user_id:** user_id of user
    - **level:** it shows user is paid subscriber or not
    - **song_id:** id of the played song
    - **artist_id:** id of artist
    - **session_id:** id of the current session
    - **location:** location of artist
    - **user_agent:** device/software to access sparkify app

##### Dimension Tables
- **users:** users in the app
    - **user_id:** unique id of user
    - **first_name:** first name of user
    - **last_name:** last name of user
    - **gender:** gender of user
    - **level:** it shows user is paid subscriber or not
- **songs:** songs in music database
    - **song_id:** unique id of song
    - **title:** name of the song
    - **artist_id:** id of artist
    - **year** year in which song is created
    - **duration:** duration of song in seconds
- **artists:** artists in music database
    - **artist_id:** unique id of artist
    - **name:** name of the artist
    - **location:** location of the artist
    - **latitude** latitude of artist location
    - **longitude:** longitude of artist location
- **time:** timestamps of records in songplays broken down into specific units
    - **start_time:** start time timestamp
    - **hour:** hour of event
    - **day:** day of event
    - **week** week of event
    - **month:** month of event
    - **year:** year of event
    - **weekday:** weekday of event

## ETL Job
A Spark job is created that fetch the logs and song json files from Amazon S3 bucket, transform it to Star Schema Data model, and saves the processed data in S3 bucket again in parquet format. This forms a data lake, as now all the data either processed or unprocessed, resides at one place for further use.
#### Data Transformations
- Song plays events are filtered based on page value as ```NextSong``` and it is joined with song data to form songplays fact data
- Timestamps are converted from UNIX time format to datetime format
- Datetime field is divided into chunks to create time dimension data

## Project Files
This project contains two files:

1. **etl.py:** reads data from S3, processes that data using Spark, and writes them back to S3
2. **dl.cfg:** config file that contains the AWS Credentials

## Project Execution
#### Steps
1. Create a bucket in AWS S3 to store the processed fact and dimension data.
2. Create an EMR instance on AWS with Spark application installed.
3. Move the Spark job file ```etl.py``` and ```dl.cfg``` to EMR instance.
4. Change the output path in Spark job file with newly created bucket in first step.
5. Change the AWS credentials in dl.cfg config file. This part can be skipped on EMR as EMR has access to S3 without credentials.
6. Submit the spark job with the following command
	```
	/usr/bin/spark-submit --master yarn etl.py
	```
7. Then check for processed data in output bucket after the completion of spark job.
8. Now the Data is present in Star Schema in parquet format and ready for use by analytics team and business people.
